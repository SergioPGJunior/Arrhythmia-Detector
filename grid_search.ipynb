{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grid_search.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYq5wYbwGYCVC2hpiI128I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergioPGJunior/detecthypotension/blob/master/grid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS-dfm243RX-"
      },
      "source": [
        "Primeiramente, é necessário excutar o comando abaixo, pois o Colab não possui o pacote wfdb instalado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EP56Qbp1f4K"
      },
      "source": [
        "! pip install wfdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHvGakOQ3bju"
      },
      "source": [
        "É realizado o import dos pacotes necessários para a organização da base de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0dS_s41piH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wfdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbfpOwrf3nzt"
      },
      "source": [
        "Em seguida, é criada uma função para carregar os sinais de ECG da base e outra função para organizar as matrizes com as amostras e os labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4Px_ba1rYi"
      },
      "source": [
        "def load_ecg(file):\n",
        "    # Leitura e armazenamento dos dados\n",
        "    record = wfdb.rdrecord(file)\n",
        "\n",
        "    # Leitura e armazenamento das anotações\n",
        "    annotation = wfdb.rdann(file, \"atr\")\n",
        "\n",
        "    #Extrai os sinais\n",
        "    p_signal = record.p_signal\n",
        "\n",
        "    #Verifica se a frequência é 360\n",
        "    assert record.fs == 360, \"Frequência de amostragem não é 360\"\n",
        "\n",
        "    #Extrai os símbolos e anotações\n",
        "    atr_sym = annotation.symbol\n",
        "    atr_sample = annotation.sample\n",
        "\n",
        "    return p_signal, atr_sym, atr_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqeVcxbn1uAQ"
      },
      "source": [
        "def build_XY(p_signal, df_ann, num_cols, abnormal, num_sec, fs):\n",
        "  # this function builds the X,Y matrices for each beat\n",
        "  # it also returns the original symbols for Y\n",
        "\n",
        "  num_rows = len(df_ann)\n",
        "  X = np.zeros((num_rows, num_cols))\n",
        "  Y = np.zeros((num_rows, 1))\n",
        "  sym = []\n",
        "\n",
        "  # keep track of rows\n",
        "  max_row = 0\n",
        "  for atr_sample, atr_sym in zip(df_ann.atr_sample.values, df_ann.atr_sym.values):\n",
        "      left = max([0, (atr_sample - num_sec * fs)])\n",
        "      right = min([len(p_signal), (atr_sample + num_sec * fs)])\n",
        "      x = p_signal[left: right]\n",
        "      if len(x) == num_cols:\n",
        "          X[max_row, :] = x\n",
        "          Y[max_row, :] = int(atr_sym in abnormal)\n",
        "          sym.append(atr_sym)\n",
        "          max_row += 1\n",
        "  X = X[:max_row, :]\n",
        "  Y = Y[:max_row, :]\n",
        "  return X, Y, sym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf4VDTzR37gC"
      },
      "source": [
        "Com o auxílio das funções criadas anteriormente, os sinais da base são carregados e as matrizes criadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQ8xceW16tU"
      },
      "source": [
        "# Caminho onde os arquivos da base de dados estão armazenados\n",
        "path = \"/content/drive/My Drive/mit-bih-arrhythmia-database-1.0.0/\"\n",
        "\n",
        "# Cria um dataframe com os nomes dos arquivos\n",
        "rec = pd.read_csv(path + \"RECORDS\", names=['n'], dtype=str)\n",
        "\n",
        "# Lista de nonbeats e abnormal\n",
        "nonbeat = ['[', '!', ']', 'x', '(', ')', 'p', 't', '`', 'u', '\\\\', '^', '|', '~', '+', 's', 'T', '*', 'D', '=', '\"', '@', 'Q', '?']\n",
        "abnormal = ['V', 'A']\n",
        "\n",
        "num_sec = 1\n",
        "fs = 360\n",
        "\n",
        "#Inicilaiza os vetores\n",
        "num_cols = 2 * num_sec * fs #numero de colunas que vai armazenar um amostra \n",
        "X_all = np.zeros((1, num_cols)) #Cria um vetor com numero de elementos igual a num_cols e preenche com zeros \n",
        "Y_all = np.zeros((1, 1)) #cria um vetor com com um elemento e preenche com zero\n",
        "sym_all = []\n",
        "\n",
        "#Lista para controlar o número de batimentos de cada paciente\n",
        "max_rows = []\n",
        "\n",
        "for pt in rec.n:\n",
        "  file = path + pt\n",
        "  p_signal, atr_sym, atr_sample = load_ecg(file)\n",
        "  \n",
        "  if 'A' and 'V' in atr_sym:\n",
        "    #Seleciona o sinal MLII\n",
        "    p_signal = p_signal[:, 0]\n",
        "    \n",
        "    # Cria um df para excluir o que mão é um batimento\n",
        "    df_ann = pd.DataFrame({'atr_sym': atr_sym,\n",
        "                          'atr_sample': atr_sample})\n",
        "    df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal + ['N'])]\n",
        "   \n",
        "    X, Y, sym = build_XY(p_signal, df_ann, num_cols, abnormal, num_sec, fs)\n",
        "    sym_all = sym_all + sym\n",
        "    max_rows.append(X.shape[0])\n",
        "    X_all = np.append(X_all, X, axis=0)\n",
        "    Y_all = np.append(Y_all, Y, axis=0)\n",
        "\n",
        "#Exclui a primeira linha que é formada por zeros\n",
        "X_all = X_all[1:, :]\n",
        "Y_all = Y_all[1:, :]\n",
        "\n",
        "#Verifica se os tamanhos das matrizes fazem sentido\n",
        "assert np.sum(max_rows) == X_all.shape[0], 'number of X, max_rows rows messed up'\n",
        "assert Y_all.shape[0] == X_all.shape[0], 'number of X, Y rows messed up'\n",
        "assert Y_all.shape[0] == len(sym_all), 'number of Y, sym rows messed up'\n",
        "\n",
        "d = {x:np.count_nonzero(Y_all == x) for x in [0,1]} #Calcula quantas amostras cada classe possui 0 -> Normal, 1 -> Arritmia\n",
        "\n",
        "print(d)\n",
        "print(len(X_all)) #Quantidade total de amostras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "967a5oW64GNz"
      },
      "source": [
        "Com o objetivo de procuar os melhores valores dos hiper-parâmetros número de neurônios e taxa de aprendizagem, é executada a busca em grade com alguns valores para esses parâmetros. Como foi utilizado um modelo de rede neural do keras e a função de busca em grade é do SkLearn, foi necessário utilizar a função KerasClassifier para \"envelopar\" a rede neural com o padrão do SKLearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ijp8VXA17gh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import utils\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_model(learn_rate=0.01, neurons=3):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(neurons, input_dim=X_all.shape[1], activation='relu'))\n",
        "  model.add(Dropout(rate=0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  opt = SGD(learning_rate=learn_rate)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model,  epochs=100, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZaRrk-02Bjs"
      },
      "source": [
        "learn_rate = [0.001,0.01,0.1]\n",
        "neurons = [3, 5, 10, 15, 20, 25, 30, 35, 40]\n",
        "\n",
        "param_grid = dict(neurons=neurons, learn_rate=learn_rate, batch_size=batch_size)\n",
        "grid = GridSearchCV(estimator=model_CV, \n",
        "                    param_grid=param_grid, n_jobs=-1,\n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_all, Y_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzS4raoc2FHf"
      },
      "source": [
        "busca_grade = pd.DataFrame(grid_result.cv_results_)\n",
        "busca_grade.to_csv(\"busca_grade.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}